"use strict";
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.adaptiveRag = adaptiveRag;
const text_splitter_1 = require("langchain/text_splitter");
const cheerio_1 = require("@langchain/community/document_loaders/web/cheerio");
const memory_1 = require("langchain/vectorstores/memory");
const hf_transformers_1 = require("@langchain/community/embeddings/hf_transformers");
const openai_1 = require("@langchain/openai");
const prompts_1 = require("@langchain/core/prompts");
const hub = __importStar(require("langchain/hub"));
const output_parsers_1 = require("@langchain/core/output_parsers");
const constants = __importStar(require("./constants"));
const langgraph_1 = require("@langchain/langgraph");
const util = __importStar(require("../utils/utils"));
const graphState = {
    question: null,
    generatedAnswer: null,
    documents: {
        value: (x, y) => y,
        default: () => [],
    },
    openAiModel: null,
};
// Checks if the model is Hallucinating or not!
// Evaluates the answer generated by the Model and the context on the basis
// of which the model was generated, to determine whether the model
// hallucinated or not.
function hallucinationGrader(state) {
    return __awaiter(this, void 0, void 0, function* () {
        const hallucinationGraderPrompt = prompts_1.ChatPromptTemplate.fromTemplate(constants.HALLUCINATION_GRADER_TEMPLATE);
        const hallucinationGrader = yield hallucinationGraderPrompt.pipe(state.openAiModel);
        return yield hallucinationGrader.invoke({
            context: util.formatDocs(state.documents),
            generation: state.generatedAnswer,
        });
    });
}
// Checks if the answer generated by the model was relevant to the question
// or not.
function answerGrader(state) {
    return __awaiter(this, void 0, void 0, function* () {
        const answerGraderPrompt = prompts_1.ChatPromptTemplate.fromTemplate(constants.ANSWER_GRADER_PROMPT_TEMPLATE);
        const answerGrader = answerGraderPrompt.pipe(state.openAiModel);
        return yield answerGrader.invoke({
            question: state.question,
            generation: state.generatedAnswer,
        });
    });
}
// Node: A dummy node to perform a web search.
const webSearch = (state) => __awaiter(void 0, void 0, void 0, function* () {
    console.log("==== Node: Performed Web Search");
    return { documents: state.documents };
});
// Node: A node to retrieve the documents.
const retrieve = (state) => __awaiter(void 0, void 0, void 0, function* () {
    console.log("==== Node: Retrieving Documents from Vector database");
    const vectorStore = yield buildVectorStore();
    const retriever = vectorStore.asRetriever();
    const retrieved_docs = yield retriever.invoke(state.question);
    return { documents: retrieved_docs };
});
// Node: A node to create OpenAI model.
const createModel = (state) => __awaiter(void 0, void 0, void 0, function* () {
    const model = new openai_1.OpenAI({
        temperature: 0,
    });
    return { openAiModel: model };
});
// Node: A node to generate answers from the LLM Model.
const generate = (state) => __awaiter(void 0, void 0, void 0, function* () {
    console.log("==== Node: Generate Answers from the LLM Model");
    // Check this to understand this prompt: 
    // https://smith.langchain.com/hub/rlm/rag-prompt
    const ragPrompt = yield hub.pull("rlm/rag-prompt");
    const ragChain = ragPrompt.pipe(state.openAiModel).pipe(new output_parsers_1.StringOutputParser());
    const generatedAnswer = yield ragChain.invoke({
        context: util.formatDocs(state.documents),
        question: state.question
    });
    console.log("Answer generated by the model: ");
    console.log(generatedAnswer);
    return { generatedAnswer };
});
// Node: A node to grade the docuemts retrieved from the Vector database.
// Checks if the question can be asnwered from the Retrieved doc.
// Evaluates the question asked and the context provided to check if the model
// can use the context doc to answer the question.
const gradeDocuments = (state) => __awaiter(void 0, void 0, void 0, function* () {
    console.log("==== Node: Grade Documents retrieved by the Vector DB");
    const relevantDocs = [];
    for (const doc of state.documents) {
        const gradePrompter = prompts_1.ChatPromptTemplate.fromTemplate(constants.GRADER_TEMPLATE);
        const retrievalGrader = gradePrompter.pipe(state.openAiModel);
        const graderResponse = yield retrievalGrader.invoke({
            question: state.question,
            content: doc.pageContent,
        });
        console.log(graderResponse.toLowerCase());
        if (graderResponse.toLowerCase().includes("yes")) {
            console.log("==== Node: Grade Documents - Relevant Doc");
            relevantDocs.push(doc);
        }
        else {
            console.log("==== Node: Grade Documents - Non-relevant Doc");
        }
    }
    return { documents: relevantDocs };
});
// Node: Transfor Query with a better one.
// This node performs query analysis on the user questions and optimizes 
// them for RAG to help handle difficult queries.
const transformQuery = (state) => __awaiter(void 0, void 0, void 0, function* () {
    console.log("==== Edge: Transform Query");
    const rewritePrompt = prompts_1.ChatPromptTemplate.fromTemplate(constants.REWRITER_PROMPT_TEMPLATE);
    const rewriter = rewritePrompt.pipe(state.openAiModel).pipe(new output_parsers_1.StringOutputParser);
    const betterQuestion = yield rewriter.invoke({
        question: state.question,
    });
    return { question: betterQuestion };
});
// Edge: Decide on the datasource to route the initial question to.
const routeQuestion = (state) => __awaiter(void 0, void 0, void 0, function* () {
    const questionRouterPrompt = prompts_1.ChatPromptTemplate.fromMessages([
        ["system", constants.QUESTION_ROUTER_SYSTEM_TEMPLATE],
        ["human", "{question}"],
    ]);
    const questionRouter = questionRouterPrompt.pipe(state.openAiModel).pipe(new output_parsers_1.StringOutputParser());
    const source = yield questionRouter.invoke({
        question: state.question,
    });
    if (source.toLowerCase().includes("web_search")) {
        return "web_search";
    }
    else {
        return "retrieve";
    }
});
// Edge: Decide whether the current documents are sufficiently relevant
// to come up with a good answer.
const decideToGenerate = (state) => __awaiter(void 0, void 0, void 0, function* () {
    const relevantDocs = state.documents;
    if (relevantDocs.length == 0) {
        console.log("==== Edge: Decide To Generate - No relevant Doc found");
        return "transform_query";
    }
    else {
        console.log("==== Edge: Decide To Generate - Relevant Doc found");
        return "generate";
    }
});
const gradeGeneratedDocumentAndQuestion = (state) => __awaiter(void 0, void 0, void 0, function* () {
    console.log("==== Edge: Grade Doc and Questions");
    const answerSupportDocuments = yield hallucinationGrader(state);
    if (answerSupportDocuments.toLowerCase().includes("yes")) {
        const answerIsRelevantToQuestion = yield answerGrader(state);
        if (answerIsRelevantToQuestion.toLowerCase().includes("yes")) {
            console.log("==== Edge: Grade Doc and Questions - Answer found relevant!");
            return "useful";
        }
        else {
            console.log("==== Edge: Grade Doc and Questions - Irrelevant answer");
            return "not_useful";
        }
    }
    else {
        console.log("==== Edge: Grade Doc and Questions - Model Hallucinating");
        return "hallucinated";
    }
});
const graph = new langgraph_1.StateGraph({ channels: graphState })
    .addNode("web_search", webSearch)
    .addNode("create_model", createModel)
    .addNode("retrieve", retrieve)
    .addNode("grade_documents", gradeDocuments)
    .addNode("generate", generate)
    .addNode("transform_query", transformQuery)
    .addEdge(langgraph_1.START, "create_model")
    .addConditionalEdges("create_model", routeQuestion)
    .addEdge("web_search", langgraph_1.END)
    .addEdge("retrieve", "grade_documents")
    .addConditionalEdges("grade_documents", decideToGenerate)
    .addEdge("transform_query", "retrieve")
    .addConditionalEdges("generate", gradeGeneratedDocumentAndQuestion, {
    hallucinated: "generate",
    useful: langgraph_1.END,
    not_useful: "transform_query",
});
const app = graph.compile({
    checkpointer: new langgraph_1.MemorySaver(),
    interruptBefore: ["web_search"],
});
function adaptiveRag() {
    return __awaiter(this, void 0, void 0, function* () {
        return yield app.invoke({
            question: "What are some features of long-term memory?",
        }, { configurable: { thread_id: "1" } });
    });
}
function buildVectorStore() {
    return __awaiter(this, void 0, void 0, function* () {
        const urls = constants.URLs;
        const docs = yield Promise.all(urls.map((url) => {
            const loader = new cheerio_1.CheerioWebBaseLoader(url);
            return loader.load();
        }));
        const docList = docs.flat();
        const textSplitter = new text_splitter_1.RecursiveCharacterTextSplitter({
            chunkSize: 250,
            chunkOverlap: 0,
        });
        const splitDocs = yield textSplitter.splitDocuments(docList);
        const embedding = new hf_transformers_1.HuggingFaceTransformersEmbeddings({
            model: "Xenova/all-MiniLM-L6-v2",
        });
        // Add to Vector store.
        return yield memory_1.MemoryVectorStore.fromDocuments(splitDocs, embedding);
    });
}
function createOpenAiModel() {
    return new openai_1.OpenAI({
        temperature: 0,
    });
}
